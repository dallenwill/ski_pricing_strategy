{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    \n",
    "# 4.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Table of Contents<a id='4.1_Table_of_Contents'></a>\n",
    "* [4.1 Table of Contents](#4.1_Table_of_Contents)\n",
    "* [4.2 Introduction](#4.2_Introduction)\n",
    "* [4.3 Library Imports](#4.3_Library_Imports)\n",
    "* [4.4 Data Loading](#4.4_Data_Loading)\n",
    "* [4.5 Feature Preparation](#4.5_Feature_Preparation)\n",
    "  * [4.5.1 Categorical Encoding](#4.5.1_Categorical_Encoding)\n",
    "  * [4.5.2 Numerical Transformation](#4.5.2_Numerical_Transformation)\n",
    "  * [4.5.3 Datetime Features](#4.5.3_Datetime_Features)\n",
    "  * [4.5.4 Text Features](#4.5.4_Text_Features)\n",
    "  * [4.5.5 Feature Creation](#4.5.5_Feature_Creation)\n",
    "  * [4.5.6 Feature Reduction](#4.5.6_Feature_Reduction)\n",
    "* [4.6 Transformer Pipeline](#4.6_Transformer_Pipeline)\n",
    "* [4.7 Summary](#4.7_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Introduction<a id='4.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create modeling-ready features from the cleaned dataset:\n",
    "- Encode categoricals\n",
    "- Scale/transform numerics\n",
    "- Extract datetime parts\n",
    "- Optional interactions\n",
    "- Prune highly correlated features\n",
    "- Build a reusable preprocessing **Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Library Imports<a id='4.3_Library_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Data Loading <a id='4.4_Data_Loading'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"../data/processed/data_02_analyzed.csv\"\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "print(\"Loaded:\", INPUT_PATH, \"| Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Feature Preparation<a id='4.5_Feature_Preparation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your target if supervised (else leave None)\n",
    "target = None  # e.g., \"price\"\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "dt_cols  = df.select_dtypes(include=['datetime', 'datetimetz']).columns.tolist()\n",
    "\n",
    "print(\"Numeric:\", len(num_cols), \"| Categorical:\", len(cat_cols), \"| Datetime:\", len(dt_cols))\n",
    "\n",
    "# Optionally parse obvious date columns by name (if read from CSV as strings)\n",
    "DATE_CANDIDATES = []  # e.g., ['date','order_date','created_at']\n",
    "for c in DATE_CANDIDATES:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "        if c not in dt_cols:\n",
    "            dt_cols.append(c)\n",
    "        if c in cat_cols:\n",
    "            cat_cols.remove(c)\n",
    "\n",
    "print(\"After date parsing -> Datetime:\", dt_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Categorical Encoding<a id='4.5.1_Categorical_Encoding'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sparse=False for wider sklearn compatibility\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", ohe)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Numerical Transformation<a id='4.5.2_Numerical_Transformation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional log transform helper (enable in pipeline if needed)\n",
    "def safe_log1p(X: np.ndarray) -> np.ndarray:\n",
    "    X = X.copy()\n",
    "    return np.log1p(np.clip(X, a_min=0, a_max=None))  # guard negatives\n",
    "\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    # (\"log\", FunctionTransformer(safe_log1p, feature_names_out=\"one-to-one\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"varthresh\", VarianceThreshold(threshold=0.0))  # drop zero-variance\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Datetime Features<a id='4.5.3_Datetime_Features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_datetime_parts(df_in: pd.DataFrame, columns):\n",
    "    df_out = df_in.copy()\n",
    "    for c in columns:\n",
    "        if c in df_out.columns:\n",
    "            s = pd.to_datetime(df_out[c], errors=\"coerce\")\n",
    "            df_out[f\"{c}_year\"]  = s.dt.year\n",
    "            df_out[f\"{c}_month\"] = s.dt.month\n",
    "            df_out[f\"{c}_day\"]   = s.dt.day\n",
    "            df_out[f\"{c}_dow\"]   = s.dt.dayofweek\n",
    "            df_out[f\"{c}_week\"]  = s.dt.isocalendar().week.astype(\"Int64\")\n",
    "    return df_out\n",
    "\n",
    "if len(dt_cols):\n",
    "    df = extract_datetime_parts(df, dt_cols)\n",
    "    for c in dt_cols:\n",
    "        for suffix in [\"year\",\"month\",\"day\",\"dow\",\"week\"]:\n",
    "            newc = f\"{c}_{suffix}\"\n",
    "            if newc in df.columns and newc not in num_cols:\n",
    "                num_cols.append(newc)\n",
    "\n",
    "print(\"Datetime-derived numeric columns added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4 Text Features<a id='4.5.4_Text_Features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TEXT_COLS = ['description']\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.5 Feature Creation<a id='4.5.5_Feature_Creation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define simple interactions you want to create: (colA, colB, new_name)\n",
    "INTERACTIONS = [\n",
    "    # (\"quantity\", \"unit_price\", \"total_price\")\n",
    "]\n",
    "for a, b, name in INTERACTIONS:\n",
    "    if a in df.columns and b in df.columns:\n",
    "        df[name] = df[a] * df[b]\n",
    "        if name not in num_cols:\n",
    "            num_cols.append(name)\n",
    "\n",
    "print(\"Interactions added:\", [n for _,_,n in INTERACTIONS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.6 Feature Reduction<a id='4.5.6_Feature_Reduction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_corr(df_in: pd.DataFrame, cols, threshold=0.95):\n",
    "    if not cols:\n",
    "        return df_in, cols, []\n",
    "    corr = df_in[cols].corr(numeric_only=True).abs()\n",
    "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "    to_drop = [c for c in upper.columns if any(upper[c] > threshold)]\n",
    "    kept = [c for c in cols if c not in to_drop]\n",
    "    return df_in.drop(columns=to_drop, errors=\"ignore\"), kept, to_drop\n",
    "\n",
    "df, num_cols, dropped_corr = drop_high_corr(df, num_cols, threshold=0.95)\n",
    "print(\"Dropped (high corr):\", dropped_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Transformer Pipeline<a id='4.6_Transformer_Pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only columns that still exist\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "cat_cols = [c for c in cat_cols if c in df.columns]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", num_pipeline, num_cols),\n",
    "        (\"cat\", cat_pipeline, cat_cols),\n",
    "        # (\"text\", tfidf, TEXT_COLS)  # if using text features\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "print(\"Numeric columns ->\", len(num_cols))\n",
    "print(\"Categorical columns ->\", len(cat_cols))\n",
    "\n",
    "# NOTE: Train/test split happens in the Modeling notebook to avoid leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Summary<a id='4.7_Summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Featured Dataset\n",
    "output_path = \"../data/processed/data_03_featured.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Featured dataset saved -> {output_path} | Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Preprocess Pipeline\n",
    "pipeline_path = \"../models/preprocess_pipeline.joblib\"\n",
    "os.makedirs(os.path.dirname(pipeline_path), exist_ok=True)\n",
    "joblib.dump(preprocess, pipeline_path)\n",
    "print(f\"✅ Preprocess pipeline saved -> {pipeline_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsenv)",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
