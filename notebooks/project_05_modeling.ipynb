{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    \n",
    "# 5.0 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Table of Contents<a id='5.1_Table_of_Contents'></a>\n",
    "* [5.1 Table of Contents](#5.1_Table_of_Contents)\n",
    "* [5.2 Introduction](#5.2_Introduction)\n",
    "* [5.3 Library Imports](#5.3_Library_Imports)\n",
    "* [5.4 Load Data & Pipeline](#5.4_Load_Data_Pipeline)\n",
    "* [5.5 Train/Test Split](#5.5_Train_Test_Split)\n",
    "* [5.6 Baseline Models](#5.6_Baseline_Models)\n",
    "* [5.7 Cross-Validation](#5.7_Cross_Validation)\n",
    "* [5.8 Model Selection (Grid Search)](#5.8_Model_Selection)\n",
    "* [5.9 Evaluation on Test Set](#5.9_Evaluation_on_Test_Set)\n",
    "* [5.10 Save Artifacts](#5.10_Save_Artifacts)\n",
    "* [5.11 Summary](#5.11_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Introduction<a id='5.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline models, run cross-validation, tune the best candidate, evaluate on a held-out test set, and save artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Library Imports<a id='5.3_Library_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 17\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Load Data & Pipeline<a id='5.4_Load_Data_Pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/processed/data_03_featured.csv\"\n",
    "PIPELINE_PATH = \"../models/preprocess_pipeline.joblib\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Data shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "TARGET = \"target_column_name\"  # <-- CHANGE THIS\n",
    "assert TARGET in df.columns, f\"Target '{TARGET}' not found in dataframe columns.\"\n",
    "\n",
    "preprocess: ColumnTransformer = joblib.load(PIPELINE_PATH)\n",
    "print(\"Loaded preprocess pipeline.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Train/Test Split<a id='5.5_Train_Test_Split'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "def infer_task_type(y_series, threshold_unique=20):\n",
    "    if y_series.dtype.name in [\"object\", \"bool\", \"category\"]:\n",
    "        return \"classification\"\n",
    "    return \"classification\" if y_series.nunique(dropna=True) <= threshold_unique else \"regression\"\n",
    "\n",
    "TASK = infer_task_type(y)\n",
    "print(\"Inferred task type:\", TASK)\n",
    "\n",
    "split_kwargs = dict(test_size=0.2, random_state=SEED)\n",
    "if TASK == \"classification\":\n",
    "    split_kwargs[\"stratify\"] = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, **split_kwargs)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Baseline Models<a id='5.6_Baseline_Models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"classification\":\n",
    "    candidates = {\n",
    "        \"logreg\": LogisticRegression(max_iter=1000),\n",
    "        \"rf_clf\": RandomForestClassifier(n_estimators=200, random_state=SEED)\n",
    "    }\n",
    "else:\n",
    "    candidates = {\n",
    "        \"linreg\": LinearRegression(),\n",
    "        \"rf_reg\": RandomForestRegressor(n_estimators=300, random_state=SEED)\n",
    "    }\n",
    "\n",
    "models = {name: Pipeline([(\"preprocess\", preprocess), (\"model\", model)])\n",
    "          for name, model in candidates.items()}\n",
    "\n",
    "print(\"Candidates:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Cross-Validation<a id='5.7_Cross_Validation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"classification\":\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scoring = \"f1_macro\"\n",
    "else:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scoring = \"neg_root_mean_squared_error\"\n",
    "\n",
    "cv_scores = {}\n",
    "for name, pipe in models.items():\n",
    "    scores = cross_val_score(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    cv_scores[name] = {\"mean\": scores.mean(), \"std\": scores.std(), \"all\": scores}\n",
    "    print(f\"{name}: {scoring} = {scores.mean():.4f} Â± {scores.std():.4f}\")\n",
    "\n",
    "best_name = max(cv_scores, key=lambda k: cv_scores[k][\"mean\"])\n",
    "print(\"Best by CV:\", best_name, \"->\", cv_scores[best_name][\"mean\"])\n",
    "best_baseline = models[best_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Model Selection (Grid Search)<a id='5.8_Model_Selection'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TASK == \"classification\" and best_name == \"rf_clf\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [200, 400],\n",
    "        \"model__max_depth\": [None, 10, 20],\n",
    "        \"model__min_samples_split\": [2, 5]\n",
    "    }\n",
    "elif TASK == \"classification\" and best_name == \"logreg\":\n",
    "    param_grid = {\n",
    "        \"model__C\": [0.5, 1.0, 2.0],\n",
    "        \"model__penalty\": [\"l2\"],\n",
    "        \"model__solver\": [\"lbfgs\"]\n",
    "    }\n",
    "elif TASK == \"regression\" and best_name == \"rf_reg\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [300, 600],\n",
    "        \"model__max_depth\": [None, 12, 24],\n",
    "        \"model__min_samples_split\": [2, 5]\n",
    "    }\n",
    "elif TASK == \"regression\" and best_name == \"linreg\":\n",
    "    param_grid = {}\n",
    "else:\n",
    "    param_grid = {}\n",
    "\n",
    "if param_grid:\n",
    "    search = GridSearchCV(best_baseline, param_grid, cv=cv, scoring=scoring, n_jobs=-1, verbose=0)\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "    print(\"Best CV score:\", search.best_score_)\n",
    "else:\n",
    "    print(\"No grid search performed (empty grid). Using best baseline.\")\n",
    "    best_model = best_baseline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Evaluation on Test Set<a id='5.9_Evaluation_on_Test_Set'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not param_grid:\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "if TASK == \"classification\":\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision(macro): {prec:.4f} | Recall(macro): {rec:.4f} | F1(macro): {f1:.4f}\")\n",
    "\n",
    "    if y_proba is not None and y_test.nunique() == 2:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "        print(f\"ROC AUC: {auc:.4f}\")\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "else:\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"MAE: {mae:.4f} | RMSE: {rmse:.4f} | R^2: {r2:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(y_pred, y_test - y_pred, s=12)\n",
    "    plt.axhline(0, ls='--')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residuals vs Predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Save Artifacts<a id='5.10_Save_Artifacts'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "MODEL_PATH = \"../models/best_model.joblib\"\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "\n",
    "report = {\"task\": TASK, \"cv_scores\": {k: {\"mean\": float(v[\"mean\"]), \"std\": float(v[\"std\"])} for k, v in cv_scores.items()}}\n",
    "if TASK == \"classification\":\n",
    "    report[\"test_metrics\"] = {\n",
    "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
    "        \"precision_macro\": float(precision_score(y_test, y_pred, average=\"macro\", zero_division=0)),\n",
    "        \"recall_macro\": float(recall_score(y_test, y_pred, average=\"macro\", zero_division=0)),\n",
    "        \"f1_macro\": float(f1_score(y_test, y_pred, average=\"macro\"))\n",
    "    }\n",
    "else:\n",
    "    report[\"test_metrics\"] = {\n",
    "        \"mae\": float(mean_absolute_error(y_test, y_pred)),\n",
    "        \"rmse\": float(mean_squared_error(y_test, y_pred, squared=False)),\n",
    "        \"r2\": float(r2_score(y_test, y_pred))\n",
    "    }\n",
    "\n",
    "REPORT_PATH = \"../models/model_report.json\"\n",
    "with open(REPORT_PATH, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "preds = pd.DataFrame({\"y_true\": y_test})\n",
    "preds[\"y_pred\"] = y_pred\n",
    "PRED_PATH = \"../data/processed/predictions_test.csv\"\n",
    "preds.to_csv(PRED_PATH, index=False)\n",
    "\n",
    "print(\"â Saved:\")\n",
    "print(\" -\", MODEL_PATH)\n",
    "print(\" -\", REPORT_PATH)\n",
    "print(\" -\", PRED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Summary<a id='5.11_Summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Verify TARGET column is correct.\n",
    "\n",
    "Compare CV vs Test metrics.\n",
    "\n",
    "If overfitting, consider regularization or feature reduction.\n",
    "\n",
    "If underfitting, explore model complexity or feature expansion.\n",
    "\n",
    "Extend grid search or test other algorithms (XGBoost, LightGBM, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dsenv)",
   "language": "python",
   "name": "dsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
